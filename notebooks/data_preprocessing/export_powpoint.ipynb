{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adapting_LLM_JSON_Extraction_Presentation.pptx'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pptx import Presentation\n",
    "from pptx.util import Inches\n",
    "\n",
    "# Create a PowerPoint presentation object\n",
    "prs = Presentation()\n",
    "\n",
    "# Function to add a slide with a title and content\n",
    "def add_slide(title, content):\n",
    "    slide_layout = prs.slide_layouts[1]  # Title and Content layout\n",
    "    slide = prs.slides.add_slide(slide_layout)\n",
    "    title_placeholder = slide.shapes.title\n",
    "    content_placeholder = slide.shapes.placeholders[1]\n",
    "    title_placeholder.text = title\n",
    "    content_placeholder.text = content\n",
    "\n",
    "# Slide 1: Title Slide\n",
    "slide_layout = prs.slide_layouts[0]  # Title Slide layout\n",
    "slide = prs.slides.add_slide(slide_layout)\n",
    "title = slide.shapes.title\n",
    "subtitle = slide.placeholders[1]\n",
    "title.text = \"Adapting Large Language Models for JSON Extraction from Text Corpora\"\n",
    "subtitle.text = \"Van-Tuan Tran, Chin-Shiuh Shieh, Ying-Chieh Chao, Casper Tsai, Mong-Fong Horng\\nNational Kaohsiung University of Science and Technology\\nIWCE 2024\"\n",
    "\n",
    "# Slide 2: Introduction\n",
    "add_slide(\"Introduction\", \n",
    "          \"- Extracting structured JSON data from unstructured text is a critical task.\\n\"\n",
    "          \"- JSON is a standard format for structured data.\\n\"\n",
    "          \"- Need for improved methods to automate JSON extraction using LLMs.\")\n",
    "\n",
    "# Slide 3: Problem Statement\n",
    "add_slide(\"Problem Statement\", \n",
    "          \"- Processing large datasets with diverse formats (HTML tags, paragraphs, irregular text).\\n\"\n",
    "          \"- Traditional rule-based methods are inefficient.\\n\"\n",
    "          \"- Gap in literature for direct application of LLMs in JSON extraction.\")\n",
    "\n",
    "# Slide 4: Research Objective\n",
    "add_slide(\"Research Objective\", \n",
    "          \"To fine-tune LLMs (Llama-2-7B, Llama-3-8B, Llama-3.1-8B) for accurate and efficient JSON extraction using QLoRA and Fully Sharded Data Parallel (FSDP).\")\n",
    "\n",
    "# Slide 5: Related Work\n",
    "add_slide(\"Related Work\", \n",
    "          \"- Rule-based methods: Require manual effort, lack scalability.\\n\"\n",
    "          \"- Pre-trained transformers (BERT, GPT-3): Focus on other NLP tasks, not JSON extraction.\\n\"\n",
    "          \"- Need for specialized fine-tuning of LLMs.\")\n",
    "\n",
    "# Slide 6: Methodology\n",
    "add_slide(\"Methodology\", \n",
    "          \"- Base models: Llama-2-7B, Llama-3-8B, Llama-3.1-8B.\\n\"\n",
    "          \"- Fine-tuning techniques: QLoRA, Fully Sharded Data Parallel (FSDP).\\n\"\n",
    "          \"- Multi-GPU setup with PyTorch and Hugging Face Transformers.\")\n",
    "\n",
    "# Slide 7: Dataset Preparation\n",
    "add_slide(\"Dataset Preparation\", \n",
    "          \"- Custom dataset combining publicly available and manually annotated data.\\n\"\n",
    "          \"- Preprocessing: Normalization, tokenization, noise filtering.\\n\"\n",
    "          \"- Dataset split into training, validation, and test sets.\")\n",
    "\n",
    "# Slide 8: Evaluation Metrics\n",
    "add_slide(\"Evaluation Metrics\", \n",
    "          \"- Accuracy (standard and normalized).\\n\"\n",
    "          \"- Loss reduction (cross-entropy loss).\\n\"\n",
    "          \"- Structural correctness of JSON outputs.\\n\"\n",
    "          \"- Tasks: ARC Challenge, HellaSwag, OpenBookQA, PIQA.\")\n",
    "\n",
    "# Slide 9: Results\n",
    "add_slide(\"Results\", \n",
    "          \"- Significant loss reduction across all models.\\n\"\n",
    "          \"- Llama-3.1-8B achieved highest accuracy (92%) in JSON extraction.\\n\"\n",
    "          \"- Faster convergence and better scalability with larger datasets.\")\n",
    "\n",
    "# Slide 10: Quantitative Metrics (Graph Placeholder)\n",
    "add_slide(\"Quantitative Metrics\", \n",
    "          \"- Loss curves for each model (Llama-2-7B, Llama-3-8B, Llama-3.1-8B).\\n\"\n",
    "          \"- Accuracy comparison across tasks (graph placeholder).\")\n",
    "\n",
    "# Slide 11: Comparative Study\n",
    "add_slide(\"Comparative Study\", \n",
    "          \"- Rule-based methods vs. LLMs: Scalability and generalization.\\n\"\n",
    "          \"- BERT and GPT-3 for NER vs. Llama for JSON extraction.\\n\"\n",
    "          \"- Llama models show superior accuracy and efficiency.\")\n",
    "\n",
    "# Slide 12: Efficiency & Scalability\n",
    "add_slide(\"Efficiency & Scalability\", \n",
    "          \"- QLoRA reduces memory usage by 30%.\\n\"\n",
    "          \"- FSDP enables efficient training on large datasets.\\n\"\n",
    "          \"- Better performance on larger datasets with minimal accuracy drop.\")\n",
    "\n",
    "# Slide 13: Conclusion\n",
    "add_slide(\"Conclusion\", \n",
    "          \"- Improved JSON extraction with fine-tuned LLMs.\\n\"\n",
    "          \"- Scalability and efficiency in real-world applications.\\n\"\n",
    "          \"- Future work: Expand datasets, optimize fine-tuning techniques.\")\n",
    "\n",
    "# Slide 14: Acknowledgements\n",
    "add_slide(\"Acknowledgements\", \n",
    "          \"Supported by National Science and Technology Council Taiwan, with grant numbers NSTC 112-2221-E-992-045, NSTC 112-2221-E-992-057-MY3, and NSTC 112-2622-8-992-009-TD1.\")\n",
    "\n",
    "# Save the presentation\n",
    "pptx_file = \"Adapting_LLM_JSON_Extraction_Presentation.pptx\"\n",
    "prs.save(pptx_file)\n",
    "\n",
    "pptx_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syner_gpus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
