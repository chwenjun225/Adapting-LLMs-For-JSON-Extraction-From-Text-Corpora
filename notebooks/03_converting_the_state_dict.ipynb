{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b657476e-1865-4687-898d-276c69eda4bc",
   "metadata": {},
   "source": [
    "# Converting the State Dict\n",
    "\n",
    "The training script (`train.py`) doesn't support any fancy saving/checkpointing methods, but it does optionally save the model right at the end of training into a safetensors file. In this notebook we'll show how to load in these saved weights for downstream evaluation and usage. This should hopefully become unneeded as frameworks integrate the changes needed to make FSDP+QLoRA work natively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a3fa90-3d40-45db-9e91-3489fc207a14",
   "metadata": {},
   "source": [
    "As an example, let's look at a model trained with the following command (using default settings for LoRA rank etc):\n",
    "\n",
    "`python train.py --save_model True --train_type qlora --output_dir qlora_output`\n",
    "\n",
    "We'll load the saved state_dict, and then copy the relevant weights into a PEFT model to save via their TODO method.\n",
    "\n",
    "Let's start by loading the state dict. If you uncomment the print statement, you'll see that for every linear layer that had a LoRA adapter, we have something like this:\n",
    "```\n",
    "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight torch.bfloat16 torch.Size([11272192, 1])\n",
    "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight torch.bfloat16 torch.Size([8, 11008])\n",
    "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight torch.bfloat16 torch.Size([4096, 8])\n",
    "```\n",
    "\n",
    "The base weights are flattened and quantized 4-bit values, which we won't need (we'll load the original base model later), and the lora_A and lora_B adapters are the ones we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4bb4b5-a250-489c-be56-5db542ac882e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "tensors = {}\n",
    "with safe_open(\"../results/model_state_dict.safetensors\", framework=\"pt\", device=0) as f:\n",
    "    for k in f.keys():\n",
    "        tensors[k] = f.get_tensor(k) # Loads the full tensor given a key\n",
    "        # print(k, tensors[k].dtype, tensors[k].shape) # Uncomment to view\n",
    "        # print(k) # Uncomment to view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a4876-c355-4b00-be1e-853de6be9ce1",
   "metadata": {},
   "source": [
    "To save memory, we can delete everything not the LoRA layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23365afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in tensors:\n",
    "    if 'lora' not in k: \n",
    "        tensors[k] = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa3483-cf79-44bf-9533-3937bd089f6e",
   "metadata": {},
   "source": [
    "Next, we load the base model and add a random adapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879e24cd-eb72-4d23-8583-12cd91ed117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Make sure the compute type, target modules, rank, alpha etc match!\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "# Load Model \n",
    "model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", use_cache=False,quantization_config=bnb_config)\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load Tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "# Freeze\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Add LoRA (make sure your rank (r) and alpha (lora_alpha) values match those used in training!)\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    inference_mode=False, r=64, \n",
    "    lora_alpha=16, lora_dropout=0.1,\n",
    "    target_modules=[\"k_proj\", \"q_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Check out the first few keys in the state dict:\n",
    "list(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322501e6-9170-4cb6-bd14-ed070045f028",
   "metadata": {},
   "source": [
    "Now, if all goes well, we can replace the randomly initialized LoRA layers with our trained ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a26eb-18d3-4e0e-b593-1bacd4987005",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sd = model.state_dict()\n",
    "\n",
    "# Create a list to store keys to remove\n",
    "keys_to_remove = []\n",
    "\n",
    "# Bên trên ta đã quantize model với QLoRA bằng `bnb_confif`, nên tới đây ta cần remove các key_quantize ra khỏi dict\n",
    "for k in new_sd:\n",
    "    if 'lora' in k:\n",
    "        new_sd[k] = tensors[k]\n",
    "    elif ('.absmax' in k) or ('.quant_state.bitsandbytes__nf4' in k) or ('.quant_map' in k): \n",
    "        keys_to_remove.append(k)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Iterate over the list of keys to remove and delete them from the dictionary\n",
    "for k in keys_to_remove:\n",
    "    del new_sd[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fff0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(new_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59ea0b-68e4-457e-9c0f-1804b327794c",
   "metadata": {},
   "source": [
    "And now, since we have a regular PEFT model, we can save using the built-in methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21096cf2-9270-478a-b7f8-9de70827c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24feebb-4928-4d1b-aa0a-b8f86e623336",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls lora_adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1300e9-4beb-47e3-ba87-3616e8cd819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub('chwenjun225/lora_adapters') # If you want to share your model... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066b5cbe",
   "metadata": {},
   "source": [
    "## Merged LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6156c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "base_model_name = \"meta-llama/Llama-2-7b-hf\" \n",
    "new_model_name = \"chwenjun225/lora_adapters\"\n",
    "\n",
    "# Make sure the compute type, target modules, rank, alpha etc match!\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model_name, \n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\", \n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "model = PeftModel.from_pretrained(base_model, new_model_name)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE `LORA MODEL`\n",
    "model.save_pretrained(\"lora_adapters\") \n",
    "tokenizer.save_pretrained(\"lora_adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b74bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"chwenjun225/lora_adapters\", token = \"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\") \n",
    "tokenizer.push_to_hub(\"chwenjun225/lora_adapters\", token = \"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\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fdf197",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0f42f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_PROMPT = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n ### Instruction: \\n{}\\n\\n### Response:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdcff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "model_inference = AutoPeftModelForCausalLM.from_pretrained(\"chwenjun225/lora_adapters\")\n",
    "tokenizer_inference = AutoTokenizer.from_pretrained(\"chwenjun225/lora_adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d76ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer_inference(\n",
    "[\n",
    "    MY_PROMPT.format(\"\"\"Extract information that you have learned from this source text:  \n",
    "MUSIC\n",
    "Pucker up! Kiss to open final 'End of the Road' tour in Cincinnati 💋\n",
    "Portrait of Luann GibbsLuann Gibbs\n",
    "Cincinnati Enquirer\n",
    "\n",
    "The final leg of the Kiss \"End of the Road\" tour begins in Cincinnati. The iconic band are wrapping up a 50-year career with a North American tour that starts at Heritage Bank Center in Cincinnati, and ends at New York City's Madison Square Garden. Tickets go on sale Friday, June 9, 2023.\n",
    "The end of the road begins in Cincinnati. The legendary rock 'n' roll band Kiss is closing out a 50-year career, but before the band packs away its iconic makeup and wild costumes, the boys are taking one last ride around the world with a final tour, fittingly titled the \"End of the Road\" tour. It will span 50 dates around the world, and the North American leg kicks off Oct. 19 right here in Cincinnati.\n",
    "\n",
    "Tickets go on sale Friday, June 9, for the show, which will take place at Heritage Bank Center (100 Broadway, Downtown). The tour wraps up in December with a massive final show at Madison Square Garden in New York City.\n",
    "\n",
    "Concert dates:Cincinnati's full 2023 concert calendar 🎵\n",
    "\n",
    "Kiss was formed in New York City in 1973 by members Paul Stanley, Gene Simmons, Ace Frehley and Peter Criss. With greasepaint makeup and outrageous costumes, the bandmembers took on the personae of comic book-style characters, and their \"shock-rock\" style live performances have been known to feature fire-breathing, blood-spitting, levitating drum kits and pyrotechnics. Considered one of the most influential rock bands of all time and one of the best-selling bands of all time, Kiss has sold more than 75 million records worldwide, earned 30 gold albums, and all four original members have been inducted into the Rock and Roll Hall of Fame.\n",
    "\n",
    "The current lineup includes Stanley, Simmons, guitarist Tommy Thayer and drummer Eric Singer.\n",
    "\n",
    "Need a break? Play the USA TODAY Daily Crossword Puzzle.\n",
    "\n",
    "Kiss 2023 North American End of the Road tour dates:\n",
    "Oct. 19: Cincinnati, Heritage Bank Center\n",
    "Oct. 20: Detroit, Little Caesars Arena\n",
    "Oct. 22: Cleveland, Rocket Mortgage FieldHouse\n",
    "Oct. 23: Nashville, Bridgestone Arena\n",
    "Oct. 25: St. Louis, Enterprise Center\n",
    "Oct. 27: Fort Worth, Texas, Dickies Arena           \n",
    "Oct. 29: Austin, Moody Center\n",
    "Nov. 1: Palm Springs, Calif. Acrisure Arena\n",
    "Nov. 3: Los Angeles, Hollywood Bowl\n",
    "Nov. 6: Seattle, Climate Pledge Arena\n",
    "Nov. 8: Vancouver, Rogers Arena\n",
    "Nov. 10: Edmonton, Alberta, Rogers Place\n",
    "Nov. 12: Calgary, Alberta, Scotiabank Saddledome\n",
    "Nov. 13: Saskatoon, Saskatchewan, SaskTel Centre\n",
    "Nov. 15: Winnipeg, Manitoba, Canada Life Centre\n",
    "Nov. 18: Montreal, Quebec, Centre Bell\n",
    "Nov. 19: Quebec, Videotron Centre\n",
    "Nov. 21: Ottawa, Ontario, Canadian Tire Centre\n",
    "Nov. 22: Toronto, Ontario, Scotiabank Arena\n",
    "Nov. 24: Knoxville, Tenn., Thompson-Boling Arena\n",
    "Nov. 25: Indianapolis, Gainbridge Fieldhouse\n",
    "Nov. 27: Rosemont, Illinois, Allstate Arena\n",
    "Nov. 29: Baltimore, CFG Bank Arena\n",
    "Dec. 1: New York City, Madison Square Garden\n",
    "Dec. 2: New York City, Madison Square Garden\"\"\")\n",
    "], return_tensors = \"pt\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer_inference)\n",
    "_ = model_inference.generate(**inputs, streamer = text_streamer, max_new_tokens=8192)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2009b4c",
   "metadata": {},
   "source": [
    "# inference.sh \n",
    "lm_eval --model hf \\\n",
    "--model_args pretrained=chwenjun225/lora_adapters,load_in_4bit=True,parallelize=True \\\n",
    "--tasks lambada_openai,hellaswag,piqa,arc_easy,arc_challenge,winogrande,openbookqa \\\n",
    "--device cuda \\\n",
    "--batch_size auto"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
